{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ce9619d-b801-43a3-8310-c0242a58e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('got through first functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e8d9313-c018-42f4-8e7a-f4204e3f84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "import h5py,pkg_resources,sys,scipy\n",
    "print('impored h5py etc')\n",
    "# import numpy as np\n",
    "# import matplotlib as mpl\n",
    "# import matplotlib.patches as patches\n",
    "# import matplotlib.image as mpimg\n",
    "# import seaborn as sns\n",
    "# import wbplot\n",
    "# from wbplot import pscalar\n",
    "# from mpl_toolkits.mplot3d import Axes3D  # This import registers the 3D projection\n",
    "# import plotly.graph_objects as go\n",
    "# # import graphviz\n",
    "# from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b42da375-d30f-4202-be04-35c939a50753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general python use\n",
    "import sys\n",
    "import os,glob,warnings,shutil\n",
    "import mne\n",
    "print('imported mne')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e94b2335-ea77-4d44-bf68-822e24ff4562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "print('imported random')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f8f1b29-e945-4566-b114-1877a2bfceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "print('imported sklearn scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b479aec-98c7-4a84-a08a-ab6ef5cdfc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal, stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import MultiComparison\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import math\n",
    "print('imported scipy sm and math')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05aae467-d8d2-4727-ba9f-a024df56ae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Decoders\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#read out matrices to files\n",
    "import io\n",
    "\n",
    "#Parallel loop\n",
    "from joblib import Parallel, delayed\n",
    "print('imported last modules')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3123e9b-6cb0-454b-87df-0dd79c5a95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_single_timeseries(classify_cond, subjNum,t, decoderType,decodingAnalysis):\n",
    "    cond_code, random_data_trl, folds_mat,random_cond_targets = DecodingAcc(classify_cond, decoderType, subjNum, decodingAnalysis)\n",
    "    nTimepoints = random_data_trl.shape[2]\n",
    "    X = random_data_trl[:, :, t]#trials x channels x 1 timepoint\n",
    "    accuracies = []\n",
    "    for train_idx, test_idx in folds_mat:\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = random_cond_targets[train_idx], random_cond_targets[test_idx]\n",
    "        \n",
    "        # Initialize a NEW classifier each time (important!)\n",
    "        if decodingAnalysis =='SVM':\n",
    "            clf = SVC(kernel='linear')\n",
    "        elif decodingAnalysis == 'LDA':\n",
    "            clf = LinearDiscriminantAnalysis()\n",
    "        elif decodingAnalysis == 'Random_Forest':\n",
    "            clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "            \n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies), sem(accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7893f29-2253-401f-b05b-9144a190b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make function to run temporal generalization \n",
    "def decode_TempGen(t_train, nTimepoints, classify_cond, subjNum, decoderType,decodingAnalysis):\n",
    "    cond_code, random_data_trl, folds_mat,random_cond_targets = DecodingAcc(classify_cond, decoderType, subjNum, decodingAnalysis)\n",
    "    nTimepoints = random_data_trl.shape[2]\n",
    "    X_train_data = random_data_trl[:, :, t_train]\n",
    "    for t in range(nTimepoints):\n",
    "        # Extract data at time t across all trials\n",
    "        # Shape: (n_trials, n_channels) --> perform classification on a time point instead a trial\n",
    "        X = random_data_trl[:, :, t]\n",
    "        # 10-fold cross-validation\n",
    "        accuracies = []\n",
    "        for train_idx, test_idx in folds:\n",
    "            X_train, X_test = X_train_data[train_idx], X[test_idx]\n",
    "            y_train, y_test = random_cond_targets[train_idx], random_cond_targets[test_idx]\n",
    "            \n",
    "            if decodingAnalysis =='SVM':\n",
    "                clf = SVC(kernel='linear')\n",
    "            elif decodingAnalysis == 'LDA':\n",
    "                clf = LinearDiscriminantAnalysis()\n",
    "            elif decodingAnalysis == 'Random_Forest':\n",
    "                clf = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            accuracies.append(acc)\n",
    "\n",
    "        # Mean cross-validated accuracy at this timepoint\n",
    "        tempGenAcc[t_train,t]=np.mean(accuracies)\n",
    "\n",
    "    return tempGenAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2d1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to load data\n",
    "def read_dataset(ds,k):\n",
    "    \"\"\"Reads an HDF5 dataset, resolving references and decoding properly.\"\"\"\n",
    "\n",
    "    # If it's a string dataset\n",
    "    str_info = h5py.check_string_dtype(ds.dtype)\n",
    "    if str_info is not None:\n",
    "        return ds.asstr()[()]\n",
    "\n",
    "    # If it's a compound structured array\n",
    "    if ds.dtype.names:\n",
    "        data = {}\n",
    "        for name in ds.dtype.names:\n",
    "            field = ds[name][()]\n",
    "            if np.issubdtype(field.dtype, np.bytes_):\n",
    "                data[name] = field.astype(str)\n",
    "            else:\n",
    "                data[name] = field\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    # If it's a dataset of references\n",
    "    if ds.dtype.kind == 'O':\n",
    "        refs = ds[()]\n",
    "        if refs.ndim > 0:\n",
    "            deref_data = []\n",
    "            for r in refs.flat:\n",
    "                if isinstance(r, h5py.h5r.Reference):\n",
    "                    real_obj = ds.file[r]\n",
    "                    if isinstance(real_obj, h5py.Dataset):\n",
    "                        deref_data.append(read_dataset(real_obj,k))\n",
    "                    elif isinstance(real_obj, h5py.Group):\n",
    "                        deref_data.append(load_group(real_obj,k))\n",
    "            return deref_data\n",
    "        else:\n",
    "            r = refs\n",
    "            real_obj = ds.file[r]\n",
    "            if isinstance(real_obj, h5py.Dataset):\n",
    "                return read_dataset(real_obj,k)\n",
    "            elif isinstance(real_obj, h5py.Group):\n",
    "                return load_group(real_obj,k)\n",
    "\n",
    "    # If it's a regular dataset\n",
    "    return ds[()]\n",
    "\n",
    "def load_group(group,k):\n",
    "    \n",
    "    \"\"\"Recursively loads an HDF5 group into a nested dictionary\"\"\"\n",
    "    result = {}\n",
    "    try:\n",
    "        for key, item in group.items():\n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                result[key] = read_dataset(item,k)\n",
    "            elif isinstance(item, h5py.Group):\n",
    "                result[key] = load_group(item,k)\n",
    "            else:\n",
    "                print(f'Unknown item type: {type(item)}')\n",
    "    except Exception as e:\n",
    "        print(f'Error {e} occurred for file {k}')\n",
    "        result[k]=read_dataset(group,k)\n",
    "    return result\n",
    "\n",
    "def loadData(dirc, file, fileName):\n",
    "\n",
    "    \"\"\"Loads structured HDF5 data safely into nested dictionaries and DataFrames\"\"\"\n",
    "    datafile = f'{dirc}{file}'\n",
    "    result = {}\n",
    "    with h5py.File(datafile, 'r') as f:\n",
    "        keys = list(f.keys())\n",
    "        print(keys)\n",
    "        for key in keys:\n",
    "            result[key] = load_group(f[key],fileName)\n",
    "    # with h5py.File(datafile, 'r') as f:\n",
    "    #     result = load_group(f)\n",
    "    print(f'Finished loading {datafile}')\n",
    "    return result\n",
    "\n",
    "def getTrialInfoCols(df, key, mk, lNum):\n",
    "\n",
    "    \"\"\"Extracts and reorders trialinfo as necessary\"\"\"\n",
    "    col = df[key][mk][lNum]\n",
    "\n",
    "    # If it's a list of arrays\n",
    "    if isinstance(col, list):\n",
    "        # Flatten and combine them\n",
    "        new_col = []\n",
    "        for c in col:\n",
    "            if isinstance(c, np.ndarray):\n",
    "                new_col.append(c.flatten()[0])  # Assuming each c is like array([[value]])\n",
    "            else:\n",
    "                new_col.append(c)\n",
    "        return new_col\n",
    "\n",
    "    # If it's a single array\n",
    "    elif isinstance(col, np.ndarray):\n",
    "        return col.flatten()\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Unexpected type {type(col)} for column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b3267d-266d-4388-9fc0-9ce283e0ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecodingAcc(classify_cond, decoderType, subjNum, decodingAnalysis):\n",
    "    #Downsampling rate -- based on Nyquist\n",
    "    downsample_rate =250\n",
    "\n",
    "    #confine to correct trials only\n",
    "    correct_only = 1\n",
    "    if correct_only==1:\n",
    "        corr_suffix = 'correctOnly'\n",
    "    else:\n",
    "        corr_suffix = []\n",
    "\n",
    "    #**run after creating pseudotrials (sub-averaging trials prior to running\n",
    "    #classification to improve SNR)\n",
    "    run_pseudotrials = 0 #0 = no pseudotrial averaging, 10 = loop over 10 repetitions of pseudotrial averaging\n",
    "    avg_pseudotrials = 14 #*set number of trials within each cond to average over\n",
    "    if run_pseudotrials == 0:\n",
    "        pseu_suffix = 'noPseudoTrials'\n",
    "    else:\n",
    "        pseu_suffix = [num2str(run_pseudotrials),'PseudoTrialsAvgOver',num2str(avg_pseudotrials),'Trials']\n",
    "\n",
    "    #Set validation type --> how are you cross-validating it\n",
    "    validation_type = '10fold'\n",
    "\n",
    "    #Response lock info\n",
    "    resp_pre = 0.5\n",
    "    resp_post = 0.5\n",
    "\n",
    "    #determine values used in classification \n",
    "    if classify_cond =='Left':\n",
    "        cond_code = [1,2]\n",
    "    elif classify_cond =='Right':\n",
    "        cond_code = [3,4]\n",
    "    else:\n",
    "        cond_code=[1,2,3,4]\n",
    "\n",
    "    #Name directories and make any necessary ones\n",
    "    runFrom = 'FND4' #fnd4 or cpro2_eeg\n",
    "    dataType = 'RawSensor' #sensor or source\n",
    "\n",
    "    if runFrom =='FND4':\n",
    "        baseDir = '/home/let83/FND4/'\n",
    "    else:\n",
    "        baseDir = '/projectsn/f_mc1689_1/cpro2_eeg/'\n",
    "    output_dir = f'{baseDir}results/DynamicDecoding/{dataType}/{decoderType}/{classify_cond}/'\n",
    "    output_file = f'_SubjectDecoding_{validation_type}_{classify_cond}_{corr_suffix}_{pseu_suffix}'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    files2load = ['fsample','elec','hdr','trialinfo','sampleinfo','trial','time','label','cfg']\n",
    "    for file in files2load:\n",
    "        input_dir = f'{baseDir}results/preproc1_causalFilter/sub{subjNum}/'\n",
    "        input_file = f'{file}_hp0.1notch_seg_autochannelz3_trialz3_ICAz3_baselinelp125avgref.mat'\n",
    "        #globals()[f'left_{key}']\n",
    "        print(file)\n",
    "        \n",
    "        try:\n",
    "            if file == 'fsample':\n",
    "                fsample = loadData(input_dir,input_file, file)\n",
    "            elif file =='elec':\n",
    "                elec = loadData(input_dir,input_file, file)\n",
    "            elif file=='hdr':\n",
    "                hdr = loadData(input_dir,input_file, file)\n",
    "            elif file =='trialinfo':\n",
    "                trialinfo = loadData(input_dir,input_file, file)\n",
    "            elif file == 'sampleinfo':\n",
    "                sampleinfo = loadData(input_dir,input_file, file)\n",
    "            elif file == 'trial':\n",
    "                trial =  loadData(input_dir,input_file, file)\n",
    "            elif file=='time':\n",
    "                time = loadData(input_dir,input_file, file)\n",
    "            elif file=='label':\n",
    "                label = loadData(input_dir,input_file, file)\n",
    "                nChans = len(label['#refs#']) - 1\n",
    "            elif file =='cfg':\n",
    "                cfg = loadData(input_dir,input_file, file)\n",
    "        except Exception as e:\n",
    "            print(f'\\nERROR: file {file} had error {e}/n')\n",
    "    print('Loaded Data')\n",
    "    #Actually Define necessary files (trialinfo, sampleinfo, time, trials)\n",
    "    #Trialinfo\n",
    "    subsystems = (trialinfo['#subsystem#'])\n",
    "    ti = {'TaskCode':[],'acc':[],'resp':[],'rt':[]}\n",
    "    colNames = ['TaskCode','acc','resp','rt']\n",
    "    i=0\n",
    "    for coln in colNames:\n",
    "        ti[coln] = getTrialInfoCols(subsystems,'MCOS',2,i)\n",
    "        i+=1\n",
    "    trialinfo = pd.DataFrame(ti)\n",
    "    print('Defined trialinfo')\n",
    "    #Trials\n",
    "    trial.keys()\n",
    "    nTimepoints = 3945\n",
    "    nTrial = 360\n",
    "    trials_real = np.zeros((nTimepoints, nChans, nTrial))\n",
    "    tkeys = list(trial['#refs#'].keys()) #Each key is a trial\n",
    "    for tnum in range(360):\n",
    "        try:\n",
    "            tk = tkeys[tnum]\n",
    "            trials_real[:,:,tnum] = trial['#refs#'][tk]\n",
    "        except Exception as e:# one row should not have the right dimensions as there are 361 keys and only 360 trials\n",
    "            print(f'ERROR {e} at {tkeys[tnum]},{tnum}!!!\\n')\n",
    "    print('Defined trials')\n",
    "    #Define samples \n",
    "    samples = pd.DataFrame({'Start':sampleinfo['sampleinfo']['sampleinfo'][0],'End':sampleinfo['sampleinfo']['sampleinfo'][1]})\n",
    "    print('defined samples')\n",
    "    #Time and trialinfo need to be in the same format\n",
    "    time_df = pd.DataFrame()\n",
    "    for i in range(360):\n",
    "        newTrial = pd.DataFrame(list(time['time']['time'][i]))\n",
    "        nt = newTrial.T\n",
    "        time_df = pd.concat([time_df,nt],ignore_index=False)\n",
    "    print('Defined time')\n",
    "    \n",
    "    #Response Lock Data\n",
    "    data_resp = {'trialinfo':trialinfo,\n",
    "                 'sampleinfo':samples, \n",
    "                 'time' : time_df,\n",
    "                 'trial': []}\n",
    "    skipped_trials = []\n",
    "    resp_secs = pd.DataFrame()\n",
    "    time_new = pd.DataFrame()\n",
    "    for t in range(len(trialinfo['rt'])):\n",
    "        start_resp=(trialinfo['rt'][t]/1000)-resp_pre #RT-500\n",
    "        end_resp=(trialinfo['rt'][t]/1000)+resp_post #RT+500\n",
    "        newRow = pd.DataFrame({'Start':[start_resp], 'End':[end_resp]})\n",
    "        # print(newRow)\n",
    "        resp_secs=pd.concat([resp_secs,newRow],ignore_index=False)\n",
    "        #find start and end inds in .time (round to deal with floating point discrepancies)\n",
    "        times = time_df.iloc[t].values\n",
    "        start_ind=np.where(np.round(times, 3) == np.round(start_resp, 3))[0]\n",
    "        end_ind = np.where(np.round(times, 3) == np.round(end_resp, 3))[0]\n",
    "\n",
    "        if len(start_ind) == 0 or len(end_ind) == 0:\n",
    "            print(f\"Warning: No matching start or end index for trial {t}. Skipping trial.\")\n",
    "            skipped_trials.append(t)\n",
    "            continue  # skip trials that weren't answered\n",
    "\n",
    "        time_row = pd.DataFrame(list(np.arange(-0.5, 0.5, 0.001))).T\n",
    "        time_new = pd.concat([time_new,time_row],ignore_index=False)\n",
    "        trl = trials_real[:, :, t]  # (channels, timepoints) for each trial\n",
    "        trl = trl[start_ind[0]:end_ind[0]+1,:]  # +1 to include endpoint like MATLAB\n",
    "\n",
    "        # Append to data_resp['trial']\n",
    "        data_resp['trial'].append(trl)\n",
    "\n",
    "    # Update the time field for each trial\n",
    "    data_resp['time'] = time_new\n",
    "    print('Response locked data')\n",
    "        \n",
    "    ##Dynamic Decoding Preproc\n",
    "    # organize trial data so it is in the format (timepoints, channels) --> (n_trials, n_channels, n_timepoints)\n",
    "    data_stack = np.stack(data_resp['trial'], axis=0) \n",
    "    data_for_resample = np.transpose(data_stack, (0, 2, 1))\n",
    "\n",
    "    # resample trials\n",
    "    resampled_data = mne.filter.resample(data_for_resample, up=1, down=4, axis=-1)\n",
    "\n",
    "    # update data_resp to reflect downsampling\n",
    "    data_resp['trial'] = resampled_data\n",
    "    print(f'Downsampled data: {resampled_data.shape}')\n",
    "    \n",
    "    ##Sort trials that will be classified\n",
    "    cond_info = trialinfo[(trialinfo['acc'] == 1) & (trialinfo['resp'].isin(cond_code))].reset_index(drop=True)\n",
    "    cond_idx = cond_info.index.tolist()\n",
    "\n",
    "    if classify_cond=='Left':\n",
    "        cond_targets = cond_info[:]['resp']\n",
    "    elif classify_cond=='Right':\n",
    "        cond_targets = []\n",
    "        for i in range(len(cond_info)):\n",
    "            if cond_info.iloc[i]['resp'] == 3:\n",
    "                cond_targets.append(1)\n",
    "            elif cond_info.iloc[i]['resp'] == 4:\n",
    "                cond_targets.append(2)\n",
    "    elif classify_cond=='Hand':\n",
    "        cond_targets = []\n",
    "        for i in range(len(cond_info)):\n",
    "            if cond_info.iloc[i]['resp'] == 3:\n",
    "                cond_targets.append(1)\n",
    "            elif cond_info.iloc[i]['resp'] == 4:\n",
    "                cond_targets.append(2)\n",
    "            elif cond_info.iloc[i]['resp']==1:\n",
    "                cond_targets.append(1)\n",
    "            else:\n",
    "                cond_targets.append(2)\n",
    "    print('defined cond_targets')\n",
    "    \n",
    "    #Time lock the data\n",
    "\n",
    "    #make/organize data necessary for mne timelocking function \n",
    "    #data (trial x channels x time)\n",
    "    #is resampled_data --> no reorganization needed\n",
    "\n",
    "    #Info abt electrodes and data collection\n",
    "    print(resampled_data.shape)\n",
    "    n_channels = resampled_data.shape[1]\n",
    "\n",
    "    print(n_channels)\n",
    "    sfreq = 250  # sampling frequency after downsampling\n",
    "    ch_names = [f'EEG {i:03d}' for i in range(n_channels)]\n",
    "    #print(ch_names)\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
    "\n",
    "    #events (trial number x dummy col x event ID)\n",
    "    n_trials = resampled_data.shape[0]\n",
    "    events = np.column_stack((np.arange(n_trials), np.zeros(n_trials, dtype=int), np.ones(n_trials, dtype=int)))\n",
    "    #Timelock data\n",
    "    data_trl = mne.EpochsArray(resampled_data, info, events=events, event_id={'response': 1}, tmin=-0.5)\n",
    "    print(f'Resampled data, new shape: {data_trl.get_data().shape}')\n",
    "    \n",
    "    xx_cond_inds = random.sample(range(len(cond_targets)), len(cond_targets))\n",
    "    \n",
    "    #Make randomized cond targets\n",
    "    random_cond_targets = []\n",
    "    for i in xx_cond_inds:\n",
    "        random_cond_targets.append(cond_targets[i])\n",
    "    print('Randomized cond_target and its indices')\n",
    "    \n",
    "    # Loop over each randomized index\n",
    "    nTimepoints = 250\n",
    "    random_data_trl = np.zeros((len(xx_cond_inds), n_channels, nTimepoints))\n",
    "    for idx in xx_cond_inds:\n",
    "        trlData = data_trl.get_data()[idx]     # pull trial data\n",
    "        random_data_trl[idx, :, :] = trlData   # final shape = (319, 251, 250)\n",
    "    print('randomized trial order within a matrix of trial data')\n",
    "    \n",
    "    #Set up Dynamic Decoding\n",
    "    #Set up basic variables\n",
    "    nFolds = 10\n",
    "    nTrials = len(xx_cond_inds)\n",
    "    \n",
    "    # Set up 10-fold Stratified Cross Validation\n",
    "    cv = StratifiedKFold(n_splits=nFolds, shuffle=True, random_state=42)\n",
    "\n",
    "    # matrix to collect accuracy info\n",
    "    n_timepoints = np.array([i for i in range(250)])\n",
    "    \n",
    "    #Determine folds before entering loop\n",
    "    X = random_data_trl[:, :, 1]\n",
    "    folds_mat = []\n",
    "    for train, test in cv.split(X, random_cond_targets):\n",
    "        folds_mat.append((train,test))\n",
    "    print('Completed setup for decoding, defined train and and test folds')\n",
    "    return cond_code, random_data_trl, folds_mat,random_cond_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1e5d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (eeg_decoding_env)",
   "language": "python",
   "name": "eeg_decoding_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
